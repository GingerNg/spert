{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 输入\n",
    "- 1. 一段文本\n",
    "- 2. entity_masks: entity_masks = pos_entity_masks + neg_entity_masks\n",
    "- 3. entity_sizes: 每个\"实体\"的len,\n",
    "- 4. entity_types 实体标签, 非实体则label=0\n",
    "\n",
    "#### 过程\n",
    "- possibilities = pos + neg\n",
    "- size_embeddings: 对entity的len进行embedding, dim=100\n",
    "\n",
    "#### 输出\n",
    "- 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True,  True, False],\n",
      "        [ True,  True,  True]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 0., 1., 1., 1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "a = [[True, True, False], [True, True, True]]\n",
    "a = torch.from_numpy(np.array(a))\n",
    "print(a)\n",
    "a.view(-1).float()  # 在pytorch中view函数的作用为重构张量的维度，相当于numpy中resize（）的功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "# from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# documents = documents[0:2]\n",
    "# documents[0][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = json.load(open(\"data/datasets/conll04/conll04_train.json\"))\n",
    "demo_docs = documents[0:50]\n",
    "json.dump(demo_docs, open(\"data/datasets/conll04/conll04_train_demo.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = json.load(open(\"data/datasets/conll04/conll04_dev.json\"))\n",
    "demo_docs = documents[0:50]\n",
    "json.dump(demo_docs, open(\"data/datasets/conll04/conll04_dev_demo.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_docs = documents[0:50]\n",
    "# json.dump(demo_docs, open(\"data/datasets/conll04/conll04_train_demo.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained(args.tokenizer_path,\n",
    "#                                             do_lower_case=args.lowercase,\n",
    "#                                             cache_dir=args.cache_path)\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "import os\n",
    "model_name = \"bert-base-chinese\"\n",
    "save_path = os.getenv(\"USERPTH\")+\"/data/huggingface/\" + model_name\n",
    "tokenizer = BertTokenizer.from_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 616.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'report', 'a', 'case', 'of', 'fulminant', 'hepatic', 'failure', 'associated', 'with', 'didanosine', 'and', 'masquerading', 'as', 'a', 'surgical', 'abdomen', 'and', 'compare', 'the', 'clinical', ',', 'biologic', ',', 'histologic', ',', 'and', 'ultrastructural', 'findings', 'with', 'reports', 'described', 'previously', '.']\n",
      "['Depressive', 'symptoms', 'disappeared', 'after', 'interferon', 'therapy', 'was', 'stopped', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 100,\n",
       " 161,\n",
       " 12267,\n",
       " 10789,\n",
       " 8810,\n",
       " 8118,\n",
       " 9796,\n",
       " 8606,\n",
       " 9586,\n",
       " 11979,\n",
       " 8303,\n",
       " 10100,\n",
       " 10673,\n",
       " 8196,\n",
       " 10967,\n",
       " 8224,\n",
       " 8174,\n",
       " 11893,\n",
       " 8179,\n",
       " 9947,\n",
       " 11467,\n",
       " 8619,\n",
       " 8168,\n",
       " 119,\n",
       " 102]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for doc in tqdm(documents):\n",
    "    jtokens = doc['tokens']\n",
    "    print(jtokens)\n",
    "    jrelations = doc['relations']\n",
    "    jentities = doc['entities']\n",
    "\n",
    "    doc_tokens = []\n",
    "\n",
    "    # full document encoding including special tokens ([CLS] and [SEP]) and byte-pair encodings of original tokens\n",
    "    doc_encoding = [tokenizer.convert_tokens_to_ids('[CLS]')]\n",
    "\n",
    "    # parse tokens\n",
    "    for i, token_phrase in enumerate(jtokens):\n",
    "        token_encoding = tokenizer.encode(token_phrase, add_special_tokens=False)\n",
    "        if not token_encoding:\n",
    "            token_encoding = [tokenizer.convert_tokens_to_ids('[UNK]')]\n",
    "        span_start, span_end = (len(doc_encoding), len(doc_encoding) + len(token_encoding))\n",
    "\n",
    "        # token = dataset.create_token(i, span_start, span_end, token_phrase)\n",
    "        token = token_phrase\n",
    "        doc_tokens.append(token)\n",
    "        doc_encoding += token_encoding\n",
    "\n",
    "    doc_encoding += [tokenizer.convert_tokens_to_ids('[SEP]')]\n",
    "\n",
    "doc_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Depressive',\n",
       " 'symptoms',\n",
       " 'disappeared',\n",
       " 'after',\n",
       " 'interferon',\n",
       " 'therapy',\n",
       " 'was',\n",
       " 'stopped',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 5, 6, 10, 9]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample negative entities\n",
    "import random\n",
    "# Chooses k unique random elements from a population sequence or set\n",
    "list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "random.sample(list, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "adfc8c81f60a3f32d13093c4442120d96ac823ba0d7eb905a8cacf8de1432327"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('spert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
